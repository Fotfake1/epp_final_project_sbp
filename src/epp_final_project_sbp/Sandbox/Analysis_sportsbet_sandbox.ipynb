{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all information needed to scrape data from football-data.co.uk\n",
    "\n",
    "\n",
    "beginning_url = \"https://www.football-data.co.uk/\"\n",
    "years = [\n",
    "    \"2223\",\n",
    "    \"2122\",\n",
    "    \"2021\",\n",
    "    \"1920\",\n",
    "    \"1819\",\n",
    "    \"1718\",\n",
    "    \"1617\",\n",
    "    \"1516\",\n",
    "    \"1415\",\n",
    "    \"1314\",\n",
    "    \"1213\",\n",
    "    \"1213\",\n",
    "    \"1112\",\n",
    "]\n",
    "Leagues = {\n",
    "    \"PL\": {\n",
    "        \"Foldername\": \"PL_data\",\n",
    "        \"Leaguetag\": \"PL\",\n",
    "        \"Leaguename\": \"E0\",\n",
    "        \"Leagueurl\": \"https://www.football-data.co.uk/englandm.php\",\n",
    "    },\n",
    "    \"BL\": {\n",
    "        \"Foldername\": \"BL_data\",\n",
    "        \"Leaguetag\": \"BL\",\n",
    "        \"Leaguename\": \"D1\",\n",
    "        \"Leagueurl\": \"https://www.football-data.co.uk/germanym.php\",\n",
    "    },\n",
    "    \"PD\": {\n",
    "        \"Foldername\": \"PD_data\",\n",
    "        \"Leaguetag\": \"PD\",\n",
    "        \"Leaguename\": \"SP1\",\n",
    "        \"Leagueurl\": \"https://www.football-data.co.uk/spainm.php\",\n",
    "    },\n",
    "    \"SA\": {\n",
    "        \"Foldername\": \"SA_data\",\n",
    "        \"Leaguetag\": \"SA\",\n",
    "        \"Leaguename\": \"I1\",\n",
    "        \"Leagueurl\": \"https://www.football-data.co.uk/italym.php\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "considered_features = [\n",
    "    \"league\",\n",
    "    \"kick_off_time\",\n",
    "    \"HomeTeam\",\n",
    "    \"AwayTeam\",\n",
    "    \"full_time_goals_hometeam\",\n",
    "    \"full_time_goals_awayteam\",\n",
    "    \"full_time_result\",\n",
    "    \"half_time_goals_hometeam\",\n",
    "    \"half_time_goals_awayteam\",\n",
    "    \"half_time_result\",\n",
    "    \"hometeam_shots\",\n",
    "    \"awayteam_shots\",\n",
    "    \"hometeam_shots_on_target\",\n",
    "    \"awayteam_shots_on_target\",\n",
    "    \"hometeam_corners\",\n",
    "    \"awayteam_corners\",\n",
    "    \"hometeam_fouls_done\",\n",
    "    \"awayteam_fouls_done\",\n",
    "    \"hometeam_yellow_cards\",\n",
    "    \"awayteam_yellow_cards\",\n",
    "    \"hometeam_red_cards\",\n",
    "    \"awayteam_red_cards\",\n",
    "    \"B365H\",\n",
    "    \"B365D\",\n",
    "    \"B365A\",\n",
    "    \"BSH\",\n",
    "    \"BSD\",\n",
    "    \"BSA\",\n",
    "    \"BWH\",\n",
    "    \"BWD\",\n",
    "    \"BWA\",\n",
    "    \"GBH\",\n",
    "    \"GBD\",\n",
    "    \"GBA\",\n",
    "    \"IWH\",\n",
    "    \"IWD\",\n",
    "    \"IWA\",\n",
    "    \"LBH\",\n",
    "    \"LBD\",\n",
    "    \"LBA\",\n",
    "    \"PSH\",\n",
    "    \"PSD\",\n",
    "    \"PSA\",\n",
    "    \"SBH\",\n",
    "    \"SBD\",\n",
    "    \"SBA\",\n",
    "    \"SJH\",\n",
    "    \"SJD\",\n",
    "    \"SJA\",\n",
    "    \"VCH\",\n",
    "    \"VCD\",\n",
    "    \"VCA\",\n",
    "    \"WHH\",\n",
    "    \"WHD\",\n",
    "    \"WHA\",\n",
    "]\n",
    "\n",
    "# all elements to make categorical\n",
    "\n",
    "categorical_features = [\n",
    "    \"league\",\n",
    "    \"HomeTeam\",\n",
    "    \"AwayTeam\",\n",
    "    \"full_time_result\",\n",
    "    \"half_time_result\",\n",
    "]\n",
    "integer_features = [\n",
    "    \"full_time_goals_hometeam\",\n",
    "    \"full_time_goals_awayteam\",\n",
    "    \"half_time_goals_hometeam\",\n",
    "    \"half_time_goals_awayteam\",\n",
    "    \"hometeam_shots\",\n",
    "    \"awayteam_shots\",\n",
    "    \"hometeam_shots_on_target\",\n",
    "    \"awayteam_shots_on_target\",\n",
    "    \"hometeam_corners\",\n",
    "    \"awayteam_corners\",\n",
    "    \"hometeam_fouls_done\",\n",
    "    \"awayteam_fouls_done\",\n",
    "    \"hometeam_yellow_cards\",\n",
    "    \"awayteam_yellow_cards\",\n",
    "    \"hometeam_red_cards\",\n",
    "    \"awayteam_red_cards\",\n",
    "]\n",
    "odd_features = [\n",
    "    \"B365H\",\n",
    "    \"B365D\",\n",
    "    \"B365A\",\n",
    "    \"BSH\",\n",
    "    \"BSD\",\n",
    "    \"BSA\",\n",
    "    \"BWH\",\n",
    "    \"BWD\",\n",
    "    \"BWA\",\n",
    "    \"GBH\",\n",
    "    \"GBD\",\n",
    "    \"GBA\",\n",
    "    \"IWH\",\n",
    "    \"IWD\",\n",
    "    \"IWA\",\n",
    "    \"LBH\",\n",
    "    \"LBD\",\n",
    "    \"LBA\",\n",
    "    \"PSH\",\n",
    "    \"PSD\",\n",
    "    \"PSA\",\n",
    "    \"SBH\",\n",
    "    \"SBD\",\n",
    "    \"SBA\",\n",
    "    \"SJH\",\n",
    "    \"SJD\",\n",
    "    \"SJA\",\n",
    "    \"VCH\",\n",
    "    \"VCD\",\n",
    "    \"VCA\",\n",
    "    \"WHH\",\n",
    "    \"WHD\",\n",
    "    \"WHA\",\n",
    "    \"consensus_odds_home\",\n",
    "    \"consensus_odds_draw\",\n",
    "    \"consensus_odds_away\",\n",
    "    \"consensus_sum_of_percentages\",\n",
    "]\n",
    "\n",
    "\n",
    "# all columns with features that are not known on game day\n",
    "not_known_on_game_day = [\n",
    "    \"full_time_goals_hometeam\",\n",
    "    \"full_time_goals_awayteam\",\n",
    "    \"half_time_goals_hometeam\",\n",
    "    \"half_time_goals_awayteam\",\n",
    "    \"half_time_result\",\n",
    "    \"hometeam_shots\",\n",
    "    \"awayteam_shots\",\n",
    "    \"hometeam_shots_on_target\",\n",
    "    \"awayteam_shots_on_target\",\n",
    "    \"hometeam_corners\",\n",
    "    \"awayteam_corners\",\n",
    "    \"hometeam_fouls_done\",\n",
    "    \"awayteam_fouls_done\",\n",
    "    \"hometeam_yellow_cards\",\n",
    "    \"awayteam_yellow_cards\",\n",
    "    \"hometeam_red_cards\",\n",
    "    \"awayteam_red_cards\",\n",
    "    \"HomeTeam_points\",\n",
    "    \"AwayTeam_points\",\n",
    "]\n",
    "odds = [\n",
    "    \"B365H\",\n",
    "    \"B365D\",\n",
    "    \"B365A\",\n",
    "    \"BSH\",\n",
    "    \"BSD\",\n",
    "    \"BSA\",\n",
    "    \"BWH\",\n",
    "    \"BWD\",\n",
    "    \"BWA\",\n",
    "    \"GBH\",\n",
    "    \"GBD\",\n",
    "    \"GBA\",\n",
    "    \"IWH\",\n",
    "    \"IWD\",\n",
    "    \"IWA\",\n",
    "    \"LBH\",\n",
    "    \"LBD\",\n",
    "    \"LBA\",\n",
    "    \"PSH\",\n",
    "    \"PSD\",\n",
    "    \"PSA\",\n",
    "    \"SBH\",\n",
    "    \"SBD\",\n",
    "    \"SBA\",\n",
    "    \"SJH\",\n",
    "    \"SJD\",\n",
    "    \"SJA\",\n",
    "    \"VCH\",\n",
    "    \"VCD\",\n",
    "    \"VCA\",\n",
    "    \"WHH\",\n",
    "    \"WHD\",\n",
    "    \"WHA\",\n",
    "    \"consensus_odds_home\",\n",
    "    \"consensus_odds_draw\",\n",
    "    \"consensus_odds_away\",\n",
    "    \"consensus_sum_of_percentages\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_consensus_odds(df, columns_with_odds):\n",
    "    \"\"\"This function computes the consensus odds\n",
    "    Input:\n",
    "        df: dataframe\n",
    "        columns_with_odds: list of columns with the odds\n",
    "    Output:\n",
    "        df: dataframe with the consensus odds added.\n",
    "    \"\"\"\n",
    "    columns_with_odds = [x for x in columns_with_odds if x in list(df.columns)]\n",
    "    home_odd_columns = [col for col in columns_with_odds if col.endswith(\"H\")]\n",
    "    draw_odd_columns = [col for col in columns_with_odds if col.endswith(\"D\")]\n",
    "    away_odd_columns = [col for col in columns_with_odds if col.endswith(\"A\")]\n",
    "\n",
    "    df[\"consensus_odds_home\"] = df[home_odd_columns].mean(axis=1)\n",
    "    df[\"consensus_odds_draw\"] = df[draw_odd_columns].mean(axis=1)\n",
    "    df[\"consensus_odds_away\"] = df[away_odd_columns].mean(axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_robustness_check(data):\n",
    "    \"\"\"Drop the columns, where all entries are NaN\n",
    "    Input:\n",
    "        data: dataframe\n",
    "    Output:\n",
    "    data: dataframe\n",
    "    .\n",
    "    \"\"\"\n",
    "    data = data.dropna(axis=1, how=\"all\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def compute_percentages_out_of_consensus_odds(df):\n",
    "    \"\"\"This function computes the percentages out of the consensus odds\n",
    "    Input:\n",
    "        df: dataframe\n",
    "        columns_with_consensus_odds: list of columns with the consensus odds\n",
    "    Output:\n",
    "        df: dataframe with the percentages out of the consensus odds added.\n",
    "    \"\"\"\n",
    "    df[\"consensus_percentage_home\"] = 1 / df[\"consensus_odds_home\"]\n",
    "    df[\"consensus_percentage_draw\"] = 1 / df[\"consensus_odds_draw\"]\n",
    "    df[\"consensus_percentage_away\"] = 1 / df[\"consensus_odds_away\"]\n",
    "    df[\"consensus_sum_of_percentages\"] = (\n",
    "        df[\"consensus_percentage_home\"]\n",
    "        + df[\"consensus_percentage_draw\"]\n",
    "        + df[\"consensus_percentage_away\"]\n",
    "    )\n",
    "    df[\"consensus_percentage_home\"] = (\n",
    "        df[\"consensus_percentage_home\"] / df[\"consensus_sum_of_percentages\"]\n",
    "    )\n",
    "    df[\"consensus_percentage_draw\"] = (\n",
    "        df[\"consensus_percentage_draw\"] / df[\"consensus_sum_of_percentages\"]\n",
    "    )\n",
    "    df[\"consensus_percentage_away\"] = (\n",
    "        df[\"consensus_percentage_away\"] / df[\"consensus_sum_of_percentages\"]\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning categorical into dummy vars\n",
    "def data_preparation(data, league, not_known_on_game_day, odds):\n",
    "    \"\"\"prepares the data, to be used in the model\n",
    "    Input:\n",
    "        data: dataframe\n",
    "        not_known_on_game_day: list of columns, which are not known on game day\n",
    "        odds: list of columns, which are the odds.\n",
    "\n",
    "    \"\"\"\n",
    "    data = data.drop(columns=\"index\")\n",
    "    data = data.set_index(\"Date\")\n",
    "    data = data.loc[data[\"league\"] == league]\n",
    "    data = compute_consensus_odds(df=data, columns_with_odds=odds)\n",
    "    data = compute_percentages_out_of_consensus_odds(df=data)\n",
    "    odds = data[list(odds)]\n",
    "    data = data.drop(columns=not_known_on_game_day)\n",
    "    data = data.drop(columns=[\"league\", \"kick_off_time\"], axis=1)\n",
    "    data = data.drop(columns=odds, axis=1)\n",
    "    data_dum = pd.get_dummies(data)\n",
    "    data_dum = data_dum.fillna(-33)\n",
    "    data_dum = data_robustness_check(data=data_dum)\n",
    "    return data_dum, odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    \"/Users/luisenriquekaiser/Documents/Final/epp_final_project_sbp/bld/python/data/data_features_added.csv\",\n",
    "    index_col=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"/Users/luisenriquekaiser/Documents/Final/epp_final_project_sbp/bld/data/processed_I1_LOGIT_model.pkl\",\n",
    "    \"rb\",\n",
    ") as f:\n",
    "    x = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"/Users/luisenriquekaiser/Documents/Final/epp_final_project_sbp/bld/data/processed_SP1_RF_model.pkl\",\n",
    "    \"rb\",\n",
    ") as f:\n",
    "    y = pickle.load(f)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dum, odds = data_preparation(\n",
    "    data=data,\n",
    "    league=\"E0\",\n",
    "    not_known_on_game_day=not_known_on_game_day,\n",
    "    odds=odds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_feature_selection_RFECV_logit(scaler, clf, min_feat, data_dum, cv_split):\n",
    "    \"\"\"computes the best feature selection for Logistic Regression\n",
    "    this function does \"\n",
    "    Input:\n",
    "        scaler: MinMaxScaler\n",
    "        clf: LogisticRegression\n",
    "        i: number of features\n",
    "        data_dum: dataframe\n",
    "        Output:\n",
    "        X_train: X_train\n",
    "        Y_train: Y_train.\n",
    "    \"\"\"\n",
    "    X_train = pd.DataFrame()\n",
    "    Y_train = pd.DataFrame()\n",
    "    model_rfecv = RFECV(\n",
    "        estimator=clf,\n",
    "        min_features_to_select=min_feat,\n",
    "        step=1,\n",
    "        cv=cv_split,\n",
    "        n_jobs=-2,\n",
    "        scoring=\"f1_macro\",\n",
    "    )\n",
    "    X_train = data_dum.drop(columns=[\"full_time_result\"])\n",
    "    Y_train = data_dum[\"full_time_result\"]\n",
    "    model_rfecv.fit(X_train, Y_train)\n",
    "    X_temp = model_rfecv.transform(X_train)\n",
    "    X_train = scaler.fit_transform(X_temp)\n",
    "    return model_rfecv, X_train, Y_train\n",
    "\n",
    "\n",
    "def cv_get_rf_model(\n",
    "    max_depth_of_trees,\n",
    "    n_bootstrap_iterations,\n",
    "    time_series_split,\n",
    "    data,\n",
    "):\n",
    "    n_estimators = [\n",
    "        int(x) for x in np.linspace(start=50, stop=n_bootstrap_iterations, num=10)\n",
    "    ]\n",
    "    max_features = [\"sqrt\"]\n",
    "    depth_of_trees = [int(x) for x in np.linspace(5, max_depth_of_trees, num=5)]\n",
    "    depth_of_trees.append(None)\n",
    "    tscv = TimeSeriesSplit(n_splits=time_series_split)\n",
    "    grid = {\n",
    "        \"n_estimators\": n_estimators,\n",
    "        \"max_features\": max_features,\n",
    "        \"max_depth\": depth_of_trees,\n",
    "        \"bootstrap\": [True],\n",
    "    }\n",
    "    X_train = data.drop(columns=[\"full_time_result\"])\n",
    "    Y_train = data[\"full_time_result\"]\n",
    "\n",
    "    rf_model_grid_search = random_forests_model(\n",
    "        split=tscv,\n",
    "        X_train=X_train,\n",
    "        Y_train=Y_train,\n",
    "        random_grid=grid,\n",
    "    )\n",
    "    # fine tune logistic regression\n",
    "\n",
    "    GridSearchCV(\n",
    "        clf,\n",
    "        parameters,\n",
    "        scoring=\"f1_macro\",\n",
    "        cv=tscv,\n",
    "        n_jobs=-2,\n",
    "        verbose=1,\n",
    "    )\n",
    "    return rf_model_grid_search\n",
    "\n",
    "\n",
    "def random_forests_model(split, X_train, Y_train, random_grid):\n",
    "    rf = RandomForestClassifier()\n",
    "    rf_model_grid_search = GridSearchCV(\n",
    "        estimator=rf,\n",
    "        param_grid=random_grid,\n",
    "        cv=split,\n",
    "        scoring=\"f1_macro\",\n",
    "        n_jobs=-2,\n",
    "    )\n",
    "    rf_model_grid_search.fit(X=X_train, y=Y_train)\n",
    "    return rf_model_grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general information\n",
    "train_share = 0.8\n",
    "\n",
    "\n",
    "def data_split(data, train_share):\n",
    "    \"\"\"Splits the data into training and test split.\n",
    "    Since the data is already sorted by date the first train_share per cent\n",
    "    are taken as a training set, the rest is the test set.\n",
    "    Input:\n",
    "        data: dataframe\n",
    "        train_share: float\n",
    "        Output:\n",
    "    train_data: dataframe\n",
    "    .\n",
    "    \"\"\"\n",
    "    train_sample = int(len(data) * train_share)\n",
    "    test_data = data.iloc[train_sample:]\n",
    "    train_data = data.iloc[:train_sample]\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "train_data, test_data = data_split(data=data_dum, train_share=train_share)\n",
    "tscv = TimeSeriesSplit(n_splits=10, max_train_size=None, test_size=None)\n",
    "\n",
    "# logit model\n",
    "min_feat = 4\n",
    "clf = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    C=0.01,\n",
    "    multi_class=\"multinomial\",\n",
    "    fit_intercept=True,\n",
    ")\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "## rf model\n",
    "max_depth_of_trees = 100\n",
    "n_bootstrap_iterations = 100\n",
    "time_series_split = 5\n",
    "data = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model, x_train_used, y_train_used = best_feature_selection_RFECV_logit(\n",
    "    scaler=scaler,\n",
    "    clf=clf,\n",
    "    min_feat=min_feat,\n",
    "    data_dum=train_data,\n",
    "    cv_split=tscv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_grid_search = cv_get_rf_model(\n",
    "    max_depth_of_trees=max_depth_of_trees,\n",
    "    n_bootstrap_iterations=n_bootstrap_iterations,\n",
    "    time_series_split=time_series_split,\n",
    "    data=train_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing best fits and time elapsed\n",
    "gs.fit(x_train_used, y_train_used)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_data.drop(columns=[\"full_time_result\"])\n",
    "y_test = test_data[\"full_time_result\"]\n",
    "x_test_logit = logit_model.transform(x_test)\n",
    "tpred_lr = gs.best_estimator_.predict(x_test_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop(columns=[\"full_time_result\"])\n",
    "Y_train = train_data[\"full_time_result\"]\n",
    "rf_model = rf_model_grid_search.best_estimator_\n",
    "rf_model.fit(X_train, Y_train)\n",
    "tpred_rf = rf_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from epp_final_project_sbp.config import INFORMATION_SCRAPING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.5483108108108109 {'C': 0.01, 'class_weight': None, 'fit_intercept': False, 'solver': 'lbfgs'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing models on unseen data\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, Y_train)\n",
    "tpred_knn = knn.predict(x_test)\n",
    "np.mean(y_test == tpred_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"/Users/luisenriquekaiser/Documents/Final/epp_final_project_sbp/bld/python/models/final_model_SP1.pkl\",\n",
    "    \"rb\",\n",
    ") as f:\n",
    "    y = pickle.load(f)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFORMATION_SCRAPING"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
